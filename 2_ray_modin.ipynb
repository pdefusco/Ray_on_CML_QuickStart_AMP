{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b016d02b-af87-4f3f-86a9-d2d971079d52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install modin[all] --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94f7b8d3-4937-40a2-8ac0-2f1d187889b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import modin.pandas as pd\n",
    "import ray\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5c457d-0905-4c97-8154-bc2e8d708353",
   "metadata": {},
   "source": [
    "If you haven't run the ray_starter notebook, uncomment the following section to create your Ray cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "365e33e7-f4e1-4633-8ac2-bbcf87ff494d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import cmlextensions.ray_cluster as rc\n",
    "\n",
    "# num_workers=2\n",
    "# head_cpu=1\n",
    "# worker_cpu=1\n",
    "# head_memory=2\n",
    "# worker_memory=2\n",
    "# worker_nvidia_gpu=0\n",
    "# head_nvidia_gpu=0\n",
    "\n",
    "# cluster = rc.RayCluster(num_workers=num_workers, worker_cpu=worker_cpu, head_cpu=head_cpu, worker_memory=worker_memory, head_memory=head_memory)\n",
    "# cluster.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85e1ea99-1df1-4c70-9142-1b57e60f68d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-09 21:11:54,647\tWARNING services.py:1832 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67108864 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=0.94gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2024-02-09 21:11:54,817\tINFO worker.py:1612 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "UserWarning: `read_*` implementation has mismatches with pandas:\n",
      "Data types of partitions are different! Please refer to the troubleshooting section of the Modin documentation to fix this issue.\n"
     ]
    }
   ],
   "source": [
    "# If needed, replace the following address with the location of your Ray cluster\n",
    "\n",
    "ray.init(address=os.getenv('RAY_CLUSTER_ADDRESS'))\n",
    "df = pd.read_csv(\"biostats.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3635c0-29fe-460d-8b84-623be8116ecb",
   "metadata": {},
   "source": [
    "NPartitions is the maximum number of splits along an axis, by default it is the number of cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "747af8bd-b329-4b3a-a3b7-3a81be7dc5a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2024-02-09 21:14:57,484 E 503 503] (raylet) node_manager.cc:3084: 8 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 1d237fa4f6823a28ba9c21346cf095cf7c53e2dc25d1c2edeffbfe80, IP: 100.100.40.84) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 100.100.40.84`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    }
   ],
   "source": [
    "from modin.config import NPartitions\n",
    "NPartitions.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9969c6-253c-4401-afaf-598eaa88b270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
