{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0cc082d-5ee6-45d1-8990-b646411a763a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install torchvision --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c537c01-2313-42d2-a58c-c48f6b178477",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from ray import tune\n",
    "from ray.air import Checkpoint, session\n",
    "from ray.tune.schedulers import ASHAScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a907d335-eeed-4f81-82fc-65cc47649dc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data(data_dir=\"./data\"):\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "    )\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(\n",
    "        root=data_dir, train=True, download=True, transform=transform\n",
    "    )\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root=data_dir, train=False, download=True, transform=transform\n",
    "    )\n",
    "\n",
    "    return trainset, testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a094e6b1-5e6c-442a-ae27-5f1840ddb28e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, l1=120, l2=84):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, l1)\n",
    "        self.fc2 = nn.Linear(l1, l2)\n",
    "        self.fc3 = nn.Linear(l2, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e57b70bf-9040-4169-8157-a3d9c3f56069",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_cifar(config, data_dir=None):\n",
    "    net = Net(config[\"l1\"], config[\"l2\"])\n",
    "\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            net = nn.DataParallel(net)\n",
    "    net.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9)\n",
    "\n",
    "    checkpoint = session.get_checkpoint()\n",
    "\n",
    "    if checkpoint:\n",
    "        checkpoint_state = checkpoint.to_dict()\n",
    "        start_epoch = checkpoint_state[\"epoch\"]\n",
    "        net.load_state_dict(checkpoint_state[\"net_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint_state[\"optimizer_state_dict\"])\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "\n",
    "    trainset, testset = load_data(data_dir)\n",
    "\n",
    "    test_abs = int(len(trainset) * 0.8)\n",
    "    train_subset, val_subset = random_split(\n",
    "        trainset, [test_abs, len(trainset) - test_abs]\n",
    "    )\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        train_subset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=8\n",
    "    )\n",
    "    valloader = torch.utils.data.DataLoader(\n",
    "        val_subset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=8\n",
    "    )\n",
    "\n",
    "    for epoch in range(start_epoch, 10):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            epoch_steps += 1\n",
    "            if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "                print(\n",
    "                    \"[%d, %5d] loss: %.3f\"\n",
    "                    % (epoch + 1, i + 1, running_loss / epoch_steps)\n",
    "                )\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for i, data in enumerate(valloader, 0):\n",
    "            with torch.no_grad():\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = net(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.cpu().numpy()\n",
    "                val_steps += 1\n",
    "\n",
    "        checkpoint_data = {\n",
    "            \"epoch\": epoch,\n",
    "            \"net_state_dict\": net.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        }\n",
    "        checkpoint = Checkpoint.from_dict(checkpoint_data)\n",
    "\n",
    "        session.report(\n",
    "            {\"loss\": val_loss / val_steps, \"accuracy\": correct / total},\n",
    "            checkpoint=checkpoint,\n",
    "        )\n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29e6321a-c207-4145-bfc2-04b2905e58d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_accuracy(net, device=\"cpu\"):\n",
    "    trainset, testset = load_data()\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=4, shuffle=False, num_workers=2\n",
    "    )\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b543f46f-a96b-48f5-94e8-b1203fdd41a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-09 00:06:28,551\tINFO tune.py:657 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "2024-02-09 00:06:28,580\tWARNING callback.py:144 -- The TensorboardX logger cannot be instantiated because either TensorboardX or one of it's dependencies is not installed. Please make sure you have the latest version of TensorboardX installed: `pip install -U tensorboardx`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-02-09 00:07:20</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:51.79        </td></tr>\n",
       "<tr><td>Memory:      </td><td>5.7/30.9 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=0<br>Bracket: Iter 2.000: None | Iter 1.000: None<br>Logical resource usage: 0/8 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 3<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                        </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_cifar_12835_00000</td><td style=\"text-align: right;\">           1</td><td>/home/cdsw/ray_results/train_cifar_2024-02-09_00-06-28/train_cifar_12835_00000_0_batch_size=2,l1=32,l2=256,lr=0.0002_2024-02-09_00-06-28/error.txt</td></tr>\n",
       "<tr><td>train_cifar_12835_00001</td><td style=\"text-align: right;\">           1</td><td>/home/cdsw/ray_results/train_cifar_2024-02-09_00-06-28/train_cifar_12835_00001_1_batch_size=2,l1=16,l2=64,lr=0.0127_2024-02-09_00-06-28/error.txt </td></tr>\n",
       "<tr><td>train_cifar_12835_00002</td><td style=\"text-align: right;\">           1</td><td>/home/cdsw/ray_results/train_cifar_2024-02-09_00-06-28/train_cifar_12835_00002_2_batch_size=4,l1=4,l2=2,lr=0.0599_2024-02-09_00-06-28/error.txt   </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  l1</th><th style=\"text-align: right;\">  l2</th><th style=\"text-align: right;\">         lr</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_cifar_12835_00000</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.000192641</td></tr>\n",
       "<tr><td>train_cifar_12835_00001</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">  16</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">0.0127375  </td></tr>\n",
       "<tr><td>train_cifar_12835_00002</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">   4</td><td style=\"text-align: right;\">   2</td><td style=\"text-align: right;\">0.0598953  </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-09 00:06:44,623\tERROR tune_controller.py:911 -- Trial task failed for trial train_cifar_12835_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cdsw/.local/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/cdsw/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/cdsw/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/cdsw/.local/lib/python3.10/site-packages/ray/_private/worker.py\", line 2495, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 100.100.202.18, ID: b11d08bcef280319c2791260fb567d7d0d7d3b5f97d69ec4bc01f9ec) where the task (task ID: ffffffffffffffff5d31ebc91743ef632d9d776301000000, name=ImplicitFunc.__init__, pid=9918, memory used=0.28GB) was running was 3.48GB / 3.66GB (0.950989), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 795c9a1b5dbae21fa014dc91143bf7e99f4e1f539d5e3afe09e86323) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 100.100.202.18`. To see the logs of the worker, use `ray logs worker-795c9a1b5dbae21fa014dc91143bf7e99f4e1f539d5e3afe09e86323*out -ip 100.100.202.18. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "856\t0.34\t/usr/local/bin/python3.10 -m ipykernel_launcher -f /tmp/kernel-0e9629d3-9d18-4f75-9994-ebd489a4a1e8....\n",
      "3928\t0.31\t/usr/local/bin/python3.10 /usr/local/bin/jedi-language-server\n",
      "9918\t0.28\tray::IDLE\n",
      "765\t0.27\t/usr/local/bin/python3.10 -m ipykernel_launcher -f /tmp/kernel-362358d7-e701-4a47-b07a-a55354838928....\n",
      "4487\t0.19\t/usr/local/bin/python3.10 /usr/local/lib/python3.10/site-packages/jedi/inference/compiled/subprocess...\n",
      "564\t0.13\t/usr/local/bin/python3.10 -m ipykernel_launcher -f /tmp/kernel-4fa23a09-a6ea-4302-80ab-961b9aad2514....\n",
      "255\t0.12\t/usr/local/bin/python3.10 -m ipykernel_launcher -f /tmp/kernel-3955536c-7b66-4101-b7d7-621e670592af....\n",
      "183\t0.11\t/usr/local/bin/python3.10 /usr/local/bin/jupyter-lab --no-browser --ip=127.0.0.1 --port=8090 --Serve...\n",
      "144\t0.08\t/usr/local/bin/python3.10 /usr/local/bin/ipython3 kernel --automagic --no-secure --nosep --pprint --...\n",
      "6672\t0.07\t/usr/local/bin/python3.10 -u /home/cdsw/.local/lib/python3.10/site-packages/ray/dashboard/agent.py -...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_cifar_12835_00000</td></tr>\n",
       "<tr><td>train_cifar_12835_00001</td></tr>\n",
       "<tr><td>train_cifar_12835_00002</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-09 00:07:03,672\tERROR tune_controller.py:911 -- Trial task failed for trial train_cifar_12835_00001\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cdsw/.local/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/cdsw/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/cdsw/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/cdsw/.local/lib/python3.10/site-packages/ray/_private/worker.py\", line 2495, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 100.100.202.18, ID: b11d08bcef280319c2791260fb567d7d0d7d3b5f97d69ec4bc01f9ec) where the task (task ID: ffffffffffffffff1f95178a00613099643a9f5a01000000, name=ImplicitFunc.__init__, pid=9962, memory used=0.33GB) was running was 3.48GB / 3.66GB (0.950055), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52a50f41ca54a342b477452b69a9f2f91a7f01fcaee7a3981f1137eb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 100.100.202.18`. To see the logs of the worker, use `ray logs worker-52a50f41ca54a342b477452b69a9f2f91a7f01fcaee7a3981f1137eb*out -ip 100.100.202.18. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "856\t0.34\t/usr/local/bin/python3.10 -m ipykernel_launcher -f /tmp/kernel-0e9629d3-9d18-4f75-9994-ebd489a4a1e8....\n",
      "9962\t0.33\tray::IDLE\n",
      "3928\t0.31\t/usr/local/bin/python3.10 /usr/local/bin/jedi-language-server\n",
      "765\t0.27\t/usr/local/bin/python3.10 -m ipykernel_launcher -f /tmp/kernel-362358d7-e701-4a47-b07a-a55354838928....\n",
      "4487\t0.19\t/usr/local/bin/python3.10 /usr/local/lib/python3.10/site-packages/jedi/inference/compiled/subprocess...\n",
      "564\t0.13\t/usr/local/bin/python3.10 -m ipykernel_launcher -f /tmp/kernel-4fa23a09-a6ea-4302-80ab-961b9aad2514....\n",
      "255\t0.12\t/usr/local/bin/python3.10 -m ipykernel_launcher -f /tmp/kernel-3955536c-7b66-4101-b7d7-621e670592af....\n",
      "183\t0.11\t/usr/local/bin/python3.10 /usr/local/bin/jupyter-lab --no-browser --ip=127.0.0.1 --port=8090 --Serve...\n",
      "144\t0.08\t/usr/local/bin/python3.10 /usr/local/bin/ipython3 kernel --automagic --no-secure --nosep --pprint --...\n",
      "6672\t0.07\t/usr/local/bin/python3.10 -u /home/cdsw/.local/lib/python3.10/site-packages/ray/dashboard/agent.py -...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2024-02-09 00:07:20,322\tERROR tune_controller.py:911 -- Trial task failed for trial train_cifar_12835_00002\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cdsw/.local/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/cdsw/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/cdsw/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/cdsw/.local/lib/python3.10/site-packages/ray/_private/worker.py\", line 2495, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 100.100.202.18, ID: b11d08bcef280319c2791260fb567d7d0d7d3b5f97d69ec4bc01f9ec) where the task (task ID: ffffffffffffffff760c96559b5f3f114a8e49d701000000, name=ImplicitFunc.__init__, pid=10009, memory used=0.31GB) was running was 3.48GB / 3.66GB (0.95053), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f6c2fb11618d527f38e5151b7f02f9a71d9fac999a2217713519826d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 100.100.202.18`. To see the logs of the worker, use `ray logs worker-f6c2fb11618d527f38e5151b7f02f9a71d9fac999a2217713519826d*out -ip 100.100.202.18. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "856\t0.34\t/usr/local/bin/python3.10 -m ipykernel_launcher -f /tmp/kernel-0e9629d3-9d18-4f75-9994-ebd489a4a1e8....\n",
      "3928\t0.31\t/usr/local/bin/python3.10 /usr/local/bin/jedi-language-server\n",
      "10009\t0.31\tray::IDLE\n",
      "765\t0.27\t/usr/local/bin/python3.10 -m ipykernel_launcher -f /tmp/kernel-362358d7-e701-4a47-b07a-a55354838928....\n",
      "4487\t0.19\t/usr/local/bin/python3.10 /usr/local/lib/python3.10/site-packages/jedi/inference/compiled/subprocess...\n",
      "564\t0.13\t/usr/local/bin/python3.10 -m ipykernel_launcher -f /tmp/kernel-4fa23a09-a6ea-4302-80ab-961b9aad2514....\n",
      "255\t0.12\t/usr/local/bin/python3.10 -m ipykernel_launcher -f /tmp/kernel-3955536c-7b66-4101-b7d7-621e670592af....\n",
      "183\t0.11\t/usr/local/bin/python3.10 /usr/local/bin/jupyter-lab --no-browser --ip=127.0.0.1 --port=8090 --Serve...\n",
      "144\t0.08\t/usr/local/bin/python3.10 /usr/local/bin/ipython3 kernel --automagic --no-secure --nosep --pprint --...\n",
      "990\t0.07\t/usr/local/bin/python3.10 -u /home/cdsw/.local/lib/python3.10/site-packages/ray/dashboard/agent.py -...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [train_cifar_12835_00000, train_cifar_12835_00001, train_cifar_12835_00002])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 51\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest trial test set accuracy: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(test_acc))\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# You can change the number of trials and GPUs here:\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_num_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpus_per_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 17\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(num_samples, max_num_epochs, gpus_per_trial)\u001b[0m\n\u001b[1;32m      4\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml1\u001b[39m\u001b[38;5;124m\"\u001b[39m: tune\u001b[38;5;241m.\u001b[39mchoice([\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mi \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m9\u001b[39m)]),\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml2\u001b[39m\u001b[38;5;124m\"\u001b[39m: tune\u001b[38;5;241m.\u001b[39mchoice([\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mi \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m9\u001b[39m)]),\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m: tune\u001b[38;5;241m.\u001b[39mloguniform(\u001b[38;5;241m1e-4\u001b[39m, \u001b[38;5;241m1e-1\u001b[39m),\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: tune\u001b[38;5;241m.\u001b[39mchoice([\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m16\u001b[39m]),\n\u001b[1;32m      9\u001b[0m }\n\u001b[1;32m     10\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m ASHAScheduler(\n\u001b[1;32m     11\u001b[0m     metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     reduction_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     16\u001b[0m )\n\u001b[0;32m---> 17\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtune\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_cifar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresources_per_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpus_per_trial\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m best_trial \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mget_best_trial(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest trial config: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_trial\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ray/tune/tune.py:1142\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, storage_path, search_alg, scheduler, checkpoint_config, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, chdir_to_trial_dir, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, reuse_actors, raise_on_failed_trial, callbacks, max_concurrent_trials, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, checkpoint_keep_all_ranks, checkpoint_upload_from_workers, trial_executor, local_dir, _experiment_checkpoint_dir, _remote, _remote_string_queue, _entrypoint)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m incomplete_trials:\n\u001b[1;32m   1141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raise_on_failed_trial \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m experiment_interrupted_event\u001b[38;5;241m.\u001b[39mis_set():\n\u001b[0;32m-> 1142\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TuneError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrials did not complete\u001b[39m\u001b[38;5;124m\"\u001b[39m, incomplete_trials)\n\u001b[1;32m   1143\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1144\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrials did not complete: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, incomplete_trials)\n",
      "\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [train_cifar_12835_00000, train_cifar_12835_00001, train_cifar_12835_00002])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2024-02-09 00:07:43,767 E 959 959] (raylet) node_manager.cc:3084: 3 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: b11d08bcef280319c2791260fb567d7d0d7d3b5f97d69ec4bc01f9ec, IP: 100.100.202.18) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 100.100.202.18`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    }
   ],
   "source": [
    "def main(num_samples=10, max_num_epochs=10, gpus_per_trial=0):\n",
    "    data_dir = os.path.abspath(\"./data\")\n",
    "    load_data(data_dir)\n",
    "    config = {\n",
    "        \"l1\": tune.choice([2**i for i in range(9)]),\n",
    "        \"l2\": tune.choice([2**i for i in range(9)]),\n",
    "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "        \"batch_size\": tune.choice([2, 4, 8, 16]),\n",
    "    }\n",
    "    scheduler = ASHAScheduler(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        max_t=max_num_epochs,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2,\n",
    "    )\n",
    "    result = tune.run(\n",
    "        partial(train_cifar, data_dir=data_dir),\n",
    "        resources_per_trial={\"cpu\": 8, \"gpu\": gpus_per_trial},\n",
    "        config=config,\n",
    "        num_samples=num_samples,\n",
    "        scheduler=scheduler,\n",
    "    )\n",
    "\n",
    "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "    print(f\"Best trial config: {best_trial.config}\")\n",
    "    print(f\"Best trial final validation loss: {best_trial.last_result['loss']}\")\n",
    "    print(f\"Best trial final validation accuracy: {best_trial.last_result['accuracy']}\")\n",
    "\n",
    "    best_trained_model = Net(best_trial.config[\"l1\"], best_trial.config[\"l2\"])\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if gpus_per_trial > 1:\n",
    "            best_trained_model = nn.DataParallel(best_trained_model)\n",
    "    best_trained_model.to(device)\n",
    "\n",
    "    best_checkpoint = best_trial.checkpoint.to_air_checkpoint()\n",
    "    best_checkpoint_data = best_checkpoint.to_dict()\n",
    "\n",
    "    best_trained_model.load_state_dict(best_checkpoint_data[\"net_state_dict\"])\n",
    "    \n",
    "    torch.save(best_trained_model.state_dict(), \"/home/cdsw/torch_model.pth\")\n",
    "\n",
    "    test_acc = test_accuracy(best_trained_model, device)\n",
    "    print(\"Best trial test set accuracy: {}\".format(test_acc))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # You can change the number of trials and GPUs here:\n",
    "    main(num_samples=3, max_num_epochs=3, gpus_per_trial=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d896ed-25ad-43eb-b164-99e6925c0307",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
